{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa0nNXaBqXLO"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "here are steps I followed:\n",
        "\n",
        "\n",
        "1. Data Inspection\n",
        "2. Outlier adjustments\n",
        "3. Feature Engineering\n",
        "4. Modeling\n",
        "5. Evalution\n",
        "6. overfitting disscussion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bl8nIUPfqycr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score\n",
        "\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGhKuwKDrAkc"
      },
      "source": [
        "## 1. Data inspection and intial analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYpVkY3nrCET"
      },
      "source": [
        "loading the dataset & inspect first 5 rows of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "K4RbcsrXrFyS",
        "outputId": "6251c78d-29a0-4739-975a-4deb73515e1e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_0</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>feature_9</th>\n",
              "      <th>feature_10</th>\n",
              "      <th>feature_11</th>\n",
              "      <th>feature_12</th>\n",
              "      <th>feature_13</th>\n",
              "      <th>feature_14</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.391943</td>\n",
              "      <td>1.386619</td>\n",
              "      <td>-0.140178</td>\n",
              "      <td>0.388745</td>\n",
              "      <td>-3.054169</td>\n",
              "      <td>-0.093261</td>\n",
              "      <td>-1.709212</td>\n",
              "      <td>2.857931</td>\n",
              "      <td>0.356881</td>\n",
              "      <td>-2.542477</td>\n",
              "      <td>0.155455</td>\n",
              "      <td>-1.306018</td>\n",
              "      <td>0.063715</td>\n",
              "      <td>1.172275</td>\n",
              "      <td>-1.302984</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.921189</td>\n",
              "      <td>-1.972131</td>\n",
              "      <td>0.073887</td>\n",
              "      <td>-1.372257</td>\n",
              "      <td>-2.172764</td>\n",
              "      <td>0.301242</td>\n",
              "      <td>1.104396</td>\n",
              "      <td>-0.048902</td>\n",
              "      <td>0.896611</td>\n",
              "      <td>4.437979</td>\n",
              "      <td>-2.222714</td>\n",
              "      <td>-2.141050</td>\n",
              "      <td>-2.381243</td>\n",
              "      <td>2.439874</td>\n",
              "      <td>1.146494</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.664802</td>\n",
              "      <td>-0.237792</td>\n",
              "      <td>1.422423</td>\n",
              "      <td>0.466005</td>\n",
              "      <td>2.051005</td>\n",
              "      <td>0.628103</td>\n",
              "      <td>-4.922358</td>\n",
              "      <td>0.204586</td>\n",
              "      <td>-2.880477</td>\n",
              "      <td>1.596624</td>\n",
              "      <td>0.843941</td>\n",
              "      <td>0.500198</td>\n",
              "      <td>-1.591305</td>\n",
              "      <td>0.053384</td>\n",
              "      <td>1.662664</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.443205</td>\n",
              "      <td>1.203531</td>\n",
              "      <td>-0.523704</td>\n",
              "      <td>1.272438</td>\n",
              "      <td>1.110078</td>\n",
              "      <td>-0.785318</td>\n",
              "      <td>-1.270623</td>\n",
              "      <td>3.521826</td>\n",
              "      <td>-3.037687</td>\n",
              "      <td>0.292205</td>\n",
              "      <td>3.863921</td>\n",
              "      <td>0.058702</td>\n",
              "      <td>1.856966</td>\n",
              "      <td>-1.365796</td>\n",
              "      <td>-0.063612</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.031723</td>\n",
              "      <td>-2.980087</td>\n",
              "      <td>-0.571903</td>\n",
              "      <td>0.527606</td>\n",
              "      <td>-4.355382</td>\n",
              "      <td>1.658870</td>\n",
              "      <td>0.028287</td>\n",
              "      <td>1.207216</td>\n",
              "      <td>-0.963811</td>\n",
              "      <td>0.245698</td>\n",
              "      <td>-0.699508</td>\n",
              "      <td>-1.290597</td>\n",
              "      <td>-2.561109</td>\n",
              "      <td>-0.641112</td>\n",
              "      <td>-0.192754</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
              "0   0.391943   1.386619  -0.140178   0.388745  -3.054169  -0.093261   \n",
              "1   0.921189  -1.972131   0.073887  -1.372257  -2.172764   0.301242   \n",
              "2   3.664802  -0.237792   1.422423   0.466005   2.051005   0.628103   \n",
              "3  -1.443205   1.203531  -0.523704   1.272438   1.110078  -0.785318   \n",
              "4  -0.031723  -2.980087  -0.571903   0.527606  -4.355382   1.658870   \n",
              "\n",
              "   feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
              "0  -1.709212   2.857931   0.356881  -2.542477    0.155455   -1.306018   \n",
              "1   1.104396  -0.048902   0.896611   4.437979   -2.222714   -2.141050   \n",
              "2  -4.922358   0.204586  -2.880477   1.596624    0.843941    0.500198   \n",
              "3  -1.270623   3.521826  -3.037687   0.292205    3.863921    0.058702   \n",
              "4   0.028287   1.207216  -0.963811   0.245698   -0.699508   -1.290597   \n",
              "\n",
              "   feature_12  feature_13  feature_14  target  \n",
              "0    0.063715    1.172275   -1.302984       1  \n",
              "1   -2.381243    2.439874    1.146494       1  \n",
              "2   -1.591305    0.053384    1.662664       1  \n",
              "3    1.856966   -1.365796   -0.063612       1  \n",
              "4   -2.561109   -0.641112   -0.192754       0  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('artifacts/dataset-1.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZdKLt1DrIjL",
        "outputId": "38229690-600c-4bf6-d2e8-ac8493060425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 16 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   feature_0   1000 non-null   float64\n",
            " 1   feature_1   1000 non-null   float64\n",
            " 2   feature_2   1000 non-null   float64\n",
            " 3   feature_3   1000 non-null   float64\n",
            " 4   feature_4   1000 non-null   float64\n",
            " 5   feature_5   1000 non-null   float64\n",
            " 6   feature_6   1000 non-null   float64\n",
            " 7   feature_7   1000 non-null   float64\n",
            " 8   feature_8   1000 non-null   float64\n",
            " 9   feature_9   1000 non-null   float64\n",
            " 10  feature_10  1000 non-null   float64\n",
            " 11  feature_11  1000 non-null   float64\n",
            " 12  feature_12  1000 non-null   float64\n",
            " 13  feature_13  1000 non-null   float64\n",
            " 14  feature_14  1000 non-null   float64\n",
            " 15  target      1000 non-null   int64  \n",
            "dtypes: float64(15), int64(1)\n",
            "memory usage: 125.1 KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "h1vtJEMtrIdM",
        "outputId": "01e6c441-70c4-4526-a773-c7e77750a048"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_0</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>feature_9</th>\n",
              "      <th>feature_10</th>\n",
              "      <th>feature_11</th>\n",
              "      <th>feature_12</th>\n",
              "      <th>feature_13</th>\n",
              "      <th>feature_14</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.018832</td>\n",
              "      <td>0.053827</td>\n",
              "      <td>0.002171</td>\n",
              "      <td>0.561165</td>\n",
              "      <td>-1.611160</td>\n",
              "      <td>-0.056954</td>\n",
              "      <td>-0.580995</td>\n",
              "      <td>0.598664</td>\n",
              "      <td>0.735379</td>\n",
              "      <td>-0.011629</td>\n",
              "      <td>-0.574440</td>\n",
              "      <td>-0.618758</td>\n",
              "      <td>-0.577339</td>\n",
              "      <td>0.054234</td>\n",
              "      <td>0.020933</td>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.201349</td>\n",
              "      <td>2.233755</td>\n",
              "      <td>0.976832</td>\n",
              "      <td>2.013021</td>\n",
              "      <td>3.833798</td>\n",
              "      <td>0.986544</td>\n",
              "      <td>1.959744</td>\n",
              "      <td>2.090510</td>\n",
              "      <td>2.265678</td>\n",
              "      <td>2.269974</td>\n",
              "      <td>3.743904</td>\n",
              "      <td>2.180531</td>\n",
              "      <td>2.130474</td>\n",
              "      <td>1.017346</td>\n",
              "      <td>2.115438</td>\n",
              "      <td>0.50025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-7.230162</td>\n",
              "      <td>-7.065339</td>\n",
              "      <td>-2.966753</td>\n",
              "      <td>-5.908949</td>\n",
              "      <td>-15.108025</td>\n",
              "      <td>-3.375579</td>\n",
              "      <td>-6.427178</td>\n",
              "      <td>-7.655322</td>\n",
              "      <td>-7.125613</td>\n",
              "      <td>-7.017417</td>\n",
              "      <td>-13.378351</td>\n",
              "      <td>-7.971723</td>\n",
              "      <td>-8.905917</td>\n",
              "      <td>-3.232565</td>\n",
              "      <td>-6.384692</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-1.473205</td>\n",
              "      <td>-1.421113</td>\n",
              "      <td>-0.680769</td>\n",
              "      <td>-0.602223</td>\n",
              "      <td>-4.168861</td>\n",
              "      <td>-0.656857</td>\n",
              "      <td>-1.902295</td>\n",
              "      <td>-0.767833</td>\n",
              "      <td>-0.737220</td>\n",
              "      <td>-1.558939</td>\n",
              "      <td>-3.068155</td>\n",
              "      <td>-2.005173</td>\n",
              "      <td>-2.030928</td>\n",
              "      <td>-0.650917</td>\n",
              "      <td>-1.375869</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.167999</td>\n",
              "      <td>0.168612</td>\n",
              "      <td>0.004955</td>\n",
              "      <td>0.718328</td>\n",
              "      <td>-1.578983</td>\n",
              "      <td>-0.077938</td>\n",
              "      <td>-0.657062</td>\n",
              "      <td>0.557735</td>\n",
              "      <td>0.812725</td>\n",
              "      <td>0.027653</td>\n",
              "      <td>-0.670406</td>\n",
              "      <td>-0.645520</td>\n",
              "      <td>-0.501724</td>\n",
              "      <td>0.054040</td>\n",
              "      <td>0.072902</td>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.567892</td>\n",
              "      <td>1.590728</td>\n",
              "      <td>0.612676</td>\n",
              "      <td>1.986342</td>\n",
              "      <td>0.946093</td>\n",
              "      <td>0.579376</td>\n",
              "      <td>0.746947</td>\n",
              "      <td>2.033430</td>\n",
              "      <td>2.313361</td>\n",
              "      <td>1.506034</td>\n",
              "      <td>1.827826</td>\n",
              "      <td>0.815433</td>\n",
              "      <td>0.993780</td>\n",
              "      <td>0.767593</td>\n",
              "      <td>1.523287</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>8.437993</td>\n",
              "      <td>7.221821</td>\n",
              "      <td>3.284118</td>\n",
              "      <td>7.701492</td>\n",
              "      <td>10.254772</td>\n",
              "      <td>3.040687</td>\n",
              "      <td>7.103293</td>\n",
              "      <td>6.565330</td>\n",
              "      <td>6.590253</td>\n",
              "      <td>8.089919</td>\n",
              "      <td>12.450599</td>\n",
              "      <td>6.689389</td>\n",
              "      <td>4.901133</td>\n",
              "      <td>3.428910</td>\n",
              "      <td>7.032418</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         feature_0    feature_1    feature_2    feature_3    feature_4  \\\n",
              "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
              "mean      0.018832     0.053827     0.002171     0.561165    -1.611160   \n",
              "std       2.201349     2.233755     0.976832     2.013021     3.833798   \n",
              "min      -7.230162    -7.065339    -2.966753    -5.908949   -15.108025   \n",
              "25%      -1.473205    -1.421113    -0.680769    -0.602223    -4.168861   \n",
              "50%       0.167999     0.168612     0.004955     0.718328    -1.578983   \n",
              "75%       1.567892     1.590728     0.612676     1.986342     0.946093   \n",
              "max       8.437993     7.221821     3.284118     7.701492    10.254772   \n",
              "\n",
              "         feature_5    feature_6    feature_7    feature_8    feature_9  \\\n",
              "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
              "mean     -0.056954    -0.580995     0.598664     0.735379    -0.011629   \n",
              "std       0.986544     1.959744     2.090510     2.265678     2.269974   \n",
              "min      -3.375579    -6.427178    -7.655322    -7.125613    -7.017417   \n",
              "25%      -0.656857    -1.902295    -0.767833    -0.737220    -1.558939   \n",
              "50%      -0.077938    -0.657062     0.557735     0.812725     0.027653   \n",
              "75%       0.579376     0.746947     2.033430     2.313361     1.506034   \n",
              "max       3.040687     7.103293     6.565330     6.590253     8.089919   \n",
              "\n",
              "        feature_10   feature_11   feature_12   feature_13   feature_14  \\\n",
              "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
              "mean     -0.574440    -0.618758    -0.577339     0.054234     0.020933   \n",
              "std       3.743904     2.180531     2.130474     1.017346     2.115438   \n",
              "min     -13.378351    -7.971723    -8.905917    -3.232565    -6.384692   \n",
              "25%      -3.068155    -2.005173    -2.030928    -0.650917    -1.375869   \n",
              "50%      -0.670406    -0.645520    -0.501724     0.054040     0.072902   \n",
              "75%       1.827826     0.815433     0.993780     0.767593     1.523287   \n",
              "max      12.450599     6.689389     4.901133     3.428910     7.032418   \n",
              "\n",
              "           target  \n",
              "count  1000.00000  \n",
              "mean      0.50000  \n",
              "std       0.50025  \n",
              "min       0.00000  \n",
              "25%       0.00000  \n",
              "50%       0.50000  \n",
              "75%       1.00000  \n",
              "max       1.00000  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-aec1jUrWAT"
      },
      "source": [
        "Notes on inital analysis:\n",
        "\n",
        "\n",
        "*   Structure: The dataset contains 1000 rows and 16 columns.\n",
        "*   Features and Target: There are 15 feature columns (feature_0 to feature_14) and one target column.\n",
        "*   Data Types: All features are numerical (float64), and the target variable is an integer (int64), which is suitable for a classification task.\n",
        "*   Missing Values: The output from df.info() shows that every column has 1000 non-null values, confirming that there are no missing values to handle.\n",
        "*   Balanced Classes: The target column has a mean of 0.5, which tells us the dataset is perfectly balanced with 500 samples for class 0 and 500 for class 1.\n",
        "*   Potential Outliers: The summary from df.describe() reveals that many features have a very wide range between their min and max values compared to their mean. specially feature_4, feature_10 This is a strong indicator that we may have outliers to address.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0SmgBnprcqY"
      },
      "source": [
        "## 2. Outlier adjustments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "cPKVcmI4rIQ8",
        "outputId": "cb7c9323-dbb0-4c91-8362-4fddb474ba87"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1152x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_copy = df.copy()\n",
        "\n",
        "def outlier_plots(data_frame, save_fig_name):\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "  fig.suptitle('Visualizing Outliers Before Handling')\n",
        "\n",
        "  # Create a box plot for feature_4\n",
        "  sns.boxplot(ax=axes[0], data=df_copy, y='feature_4')\n",
        "  axes[0].set_title('Box Plot of feature_4')\n",
        "\n",
        "  # Create a box plot for feature_10\n",
        "  sns.boxplot(ax=axes[1], data=df_copy, y='feature_10')\n",
        "  axes[1].set_title('Box Plot of feature_10')\n",
        "\n",
        "  plt.savefig(save_fig_name)\n",
        "  plt.show()\n",
        "\n",
        "  return\n",
        "\n",
        "outlier_plots(df_copy, 'outlier_boxplots_before.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f3Pez6PrnHH"
      },
      "source": [
        "the **Interquartile Range (IQR)** method for handling outliers primarily. It effectively mitigates the influence of extreme values without making risky assumptions about our data's distribution and without forcing us to delete valuable data. It's a robust, reliable, and straightforward first step in cleaning our dataset for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "EDwNlUlmrjO9",
        "outputId": "95cba0d8-2956-4e37-8b7b-8f20462323fc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_0</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>feature_9</th>\n",
              "      <th>feature_10</th>\n",
              "      <th>feature_11</th>\n",
              "      <th>feature_12</th>\n",
              "      <th>feature_13</th>\n",
              "      <th>feature_14</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.018697</td>\n",
              "      <td>0.057552</td>\n",
              "      <td>0.001266</td>\n",
              "      <td>0.567886</td>\n",
              "      <td>-1.609294</td>\n",
              "      <td>-0.055641</td>\n",
              "      <td>-0.586448</td>\n",
              "      <td>0.602097</td>\n",
              "      <td>0.738330</td>\n",
              "      <td>-0.015233</td>\n",
              "      <td>-0.579265</td>\n",
              "      <td>-0.617005</td>\n",
              "      <td>-0.572319</td>\n",
              "      <td>0.053855</td>\n",
              "      <td>0.020888</td>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.187281</td>\n",
              "      <td>2.216415</td>\n",
              "      <td>0.969710</td>\n",
              "      <td>1.980866</td>\n",
              "      <td>3.795543</td>\n",
              "      <td>0.970309</td>\n",
              "      <td>1.936367</td>\n",
              "      <td>2.075628</td>\n",
              "      <td>2.256939</td>\n",
              "      <td>2.253735</td>\n",
              "      <td>3.703945</td>\n",
              "      <td>2.148902</td>\n",
              "      <td>2.114335</td>\n",
              "      <td>1.010030</td>\n",
              "      <td>2.107349</td>\n",
              "      <td>0.50025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-6.034851</td>\n",
              "      <td>-5.938873</td>\n",
              "      <td>-2.620936</td>\n",
              "      <td>-4.485070</td>\n",
              "      <td>-11.841291</td>\n",
              "      <td>-2.511207</td>\n",
              "      <td>-5.876159</td>\n",
              "      <td>-4.969728</td>\n",
              "      <td>-5.313092</td>\n",
              "      <td>-6.156400</td>\n",
              "      <td>-10.412126</td>\n",
              "      <td>-6.236081</td>\n",
              "      <td>-6.567991</td>\n",
              "      <td>-2.778682</td>\n",
              "      <td>-5.724603</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-1.473205</td>\n",
              "      <td>-1.421113</td>\n",
              "      <td>-0.680769</td>\n",
              "      <td>-0.602223</td>\n",
              "      <td>-4.168861</td>\n",
              "      <td>-0.656857</td>\n",
              "      <td>-1.902295</td>\n",
              "      <td>-0.767833</td>\n",
              "      <td>-0.737220</td>\n",
              "      <td>-1.558939</td>\n",
              "      <td>-3.068155</td>\n",
              "      <td>-2.005173</td>\n",
              "      <td>-2.030928</td>\n",
              "      <td>-0.650917</td>\n",
              "      <td>-1.375869</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.167999</td>\n",
              "      <td>0.168612</td>\n",
              "      <td>0.004955</td>\n",
              "      <td>0.718328</td>\n",
              "      <td>-1.578983</td>\n",
              "      <td>-0.077938</td>\n",
              "      <td>-0.657062</td>\n",
              "      <td>0.557735</td>\n",
              "      <td>0.812725</td>\n",
              "      <td>0.027653</td>\n",
              "      <td>-0.670406</td>\n",
              "      <td>-0.645520</td>\n",
              "      <td>-0.501724</td>\n",
              "      <td>0.054040</td>\n",
              "      <td>0.072902</td>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.567892</td>\n",
              "      <td>1.590728</td>\n",
              "      <td>0.612676</td>\n",
              "      <td>1.986342</td>\n",
              "      <td>0.946093</td>\n",
              "      <td>0.579376</td>\n",
              "      <td>0.746947</td>\n",
              "      <td>2.033430</td>\n",
              "      <td>2.313361</td>\n",
              "      <td>1.506034</td>\n",
              "      <td>1.827826</td>\n",
              "      <td>0.815433</td>\n",
              "      <td>0.993780</td>\n",
              "      <td>0.767593</td>\n",
              "      <td>1.523287</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>6.129538</td>\n",
              "      <td>6.108488</td>\n",
              "      <td>2.552843</td>\n",
              "      <td>5.869189</td>\n",
              "      <td>8.618523</td>\n",
              "      <td>2.433726</td>\n",
              "      <td>4.720811</td>\n",
              "      <td>6.235324</td>\n",
              "      <td>6.590253</td>\n",
              "      <td>6.103494</td>\n",
              "      <td>9.171797</td>\n",
              "      <td>5.046341</td>\n",
              "      <td>4.901133</td>\n",
              "      <td>2.895357</td>\n",
              "      <td>5.872022</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         feature_0    feature_1    feature_2    feature_3    feature_4  \\\n",
              "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
              "mean      0.018697     0.057552     0.001266     0.567886    -1.609294   \n",
              "std       2.187281     2.216415     0.969710     1.980866     3.795543   \n",
              "min      -6.034851    -5.938873    -2.620936    -4.485070   -11.841291   \n",
              "25%      -1.473205    -1.421113    -0.680769    -0.602223    -4.168861   \n",
              "50%       0.167999     0.168612     0.004955     0.718328    -1.578983   \n",
              "75%       1.567892     1.590728     0.612676     1.986342     0.946093   \n",
              "max       6.129538     6.108488     2.552843     5.869189     8.618523   \n",
              "\n",
              "         feature_5    feature_6    feature_7    feature_8    feature_9  \\\n",
              "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
              "mean     -0.055641    -0.586448     0.602097     0.738330    -0.015233   \n",
              "std       0.970309     1.936367     2.075628     2.256939     2.253735   \n",
              "min      -2.511207    -5.876159    -4.969728    -5.313092    -6.156400   \n",
              "25%      -0.656857    -1.902295    -0.767833    -0.737220    -1.558939   \n",
              "50%      -0.077938    -0.657062     0.557735     0.812725     0.027653   \n",
              "75%       0.579376     0.746947     2.033430     2.313361     1.506034   \n",
              "max       2.433726     4.720811     6.235324     6.590253     6.103494   \n",
              "\n",
              "        feature_10   feature_11   feature_12   feature_13   feature_14  \\\n",
              "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
              "mean     -0.579265    -0.617005    -0.572319     0.053855     0.020888   \n",
              "std       3.703945     2.148902     2.114335     1.010030     2.107349   \n",
              "min     -10.412126    -6.236081    -6.567991    -2.778682    -5.724603   \n",
              "25%      -3.068155    -2.005173    -2.030928    -0.650917    -1.375869   \n",
              "50%      -0.670406    -0.645520    -0.501724     0.054040     0.072902   \n",
              "75%       1.827826     0.815433     0.993780     0.767593     1.523287   \n",
              "max       9.171797     5.046341     4.901133     2.895357     5.872022   \n",
              "\n",
              "           target  \n",
              "count  1000.00000  \n",
              "mean      0.50000  \n",
              "std       0.50025  \n",
              "min       0.00000  \n",
              "25%       0.00000  \n",
              "50%       0.50000  \n",
              "75%       1.00000  \n",
              "max       1.00000  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features_to_clean = df_copy.columns.drop('target')\n",
        "\n",
        "for feature in features_to_clean:\n",
        "    # Calculate the first quartile (Q1)\n",
        "    Q1 = df_copy[feature].quantile(0.25)\n",
        "\n",
        "    # Calculate the third quartile (Q3)\n",
        "    Q3 = df_copy[feature].quantile(0.75)\n",
        "\n",
        "    # Calculate the Interquartile Range (IQR)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Define the lower and upper bounds for outlier detection\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Cap the outliers\n",
        "    df_copy[feature] = np.where(df_copy[feature] < lower_bound, lower_bound, df_copy[feature])\n",
        "    df_copy[feature] = np.where(df_copy[feature] > upper_bound, upper_bound, df_copy[feature])\n",
        "\n",
        "df_copy.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXTSAKXwrujn"
      },
      "source": [
        "Comparing the describe() output from before and after capping clearly shows our success. For example, let's look at feature_4:\n",
        "\n",
        "* Before: The min was -15.11 and the max was 10.25.\n",
        "* After: The min is now -11.84 and the max is 8.62.\n",
        "\n",
        "The extreme values have been reined in, which will help create a more robust and stable model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "IZL5g49CrjH7",
        "outputId": "352def1f-307e-4e37-cfb4-ad0f11f763f7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1152x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "outlier_plots(df_copy, 'outlier_boxplots_after.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcA59nU3r2Fk"
      },
      "source": [
        "we can confirm visually, by running the box plot code on df_copy, and we would see no points outside the whiskers.\n",
        "\n",
        "now saving the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXCcLu6crjB4",
        "outputId": "20f81c9c-87d8-49a9-9514-ea7d5113bb8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned dataset saved to 'dataset_cleaned.csv'\n"
          ]
        }
      ],
      "source": [
        "df_copy.to_csv('dataset_cleaned.csv', index=False)\n",
        "\n",
        "print(\"Cleaned dataset saved to 'dataset_cleaned.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2gMv3SYsA2O"
      },
      "source": [
        "## 3. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCcOwcofsAYb",
        "outputId": "75c3b846-fa85-4a4c-d9d7-c79bab485e7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correlation of each feature with the target variable:\n",
            "target        1.000000\n",
            "feature_3     0.309822\n",
            "feature_7     0.279585\n",
            "feature_12    0.269907\n",
            "feature_11    0.231804\n",
            "feature_10    0.158396\n",
            "feature_13    0.086531\n",
            "feature_2     0.036025\n",
            "feature_0     0.002695\n",
            "feature_5    -0.001420\n",
            "feature_1    -0.017931\n",
            "feature_4    -0.039213\n",
            "feature_14   -0.042420\n",
            "feature_6    -0.262881\n",
            "feature_8    -0.281113\n",
            "feature_9    -0.499866\n",
            "Name: target, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df = pd.read_csv('dataset_cleaned.csv')\n",
        "correlation_matrix = df.corr()\n",
        "corr_with_target = correlation_matrix['target'].sort_values(ascending=False)\n",
        "\n",
        "print(\"Correlation of each feature with the target variable:\")\n",
        "print(corr_with_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig7lzJATsI2Z"
      },
      "source": [
        "**Analysis of Your Correlation Output:**\n",
        "\n",
        "Above results show that:\n",
        "\n",
        "* The features with the strongest positive correlation to the *target* are *feature_3*, *feature_7*, and *feature_12*.\n",
        "\n",
        "* The feature with the strongest negative correlation is *feature_9* by a large margin, followed by *feature_8* and *feature_6*.\n",
        "\n",
        "\n",
        "This leads us to a much more powerful and justifiable feature engineering plan:\n",
        "\n",
        "1. **positive_predictors_sum:** We'll sum the top three positive predictors *(feature_3, feature_7, feature_12)*. This combines their predictive power into a single, strong feature.\n",
        "\n",
        "2. **main_interaction:** We'll multiply the strongest positive predictor *(feature_3)* with the strongest negative predictor *(feature_9)*. This creates an interaction term that captures the tension between the most significant positive and negative forces acting on our target.\n",
        "\n",
        "The entire purpose of these features is to amplify signals that are **already known to be predictive of the target** variable. We are explicitly helping the model by combining features that have a demonstrated relationship with the outcome we want to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "pFrvVvjGri08",
        "outputId": "139895a0-1238-420b-e098-d8149c908429"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully created two new, data-driven features.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_0</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>feature_9</th>\n",
              "      <th>feature_10</th>\n",
              "      <th>feature_11</th>\n",
              "      <th>feature_12</th>\n",
              "      <th>feature_13</th>\n",
              "      <th>feature_14</th>\n",
              "      <th>target</th>\n",
              "      <th>positive_predictors_sum</th>\n",
              "      <th>main_interaction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.391943</td>\n",
              "      <td>1.386619</td>\n",
              "      <td>-0.140178</td>\n",
              "      <td>0.388745</td>\n",
              "      <td>-3.054169</td>\n",
              "      <td>-0.093261</td>\n",
              "      <td>-1.709212</td>\n",
              "      <td>2.857931</td>\n",
              "      <td>0.356881</td>\n",
              "      <td>-2.542477</td>\n",
              "      <td>0.155455</td>\n",
              "      <td>-1.306018</td>\n",
              "      <td>0.063715</td>\n",
              "      <td>1.172275</td>\n",
              "      <td>-1.302984</td>\n",
              "      <td>1</td>\n",
              "      <td>3.310391</td>\n",
              "      <td>-0.988375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.921189</td>\n",
              "      <td>-1.972131</td>\n",
              "      <td>0.073887</td>\n",
              "      <td>-1.372257</td>\n",
              "      <td>-2.172764</td>\n",
              "      <td>0.301242</td>\n",
              "      <td>1.104396</td>\n",
              "      <td>-0.048902</td>\n",
              "      <td>0.896611</td>\n",
              "      <td>4.437979</td>\n",
              "      <td>-2.222714</td>\n",
              "      <td>-2.141050</td>\n",
              "      <td>-2.381243</td>\n",
              "      <td>2.439874</td>\n",
              "      <td>1.146494</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.802402</td>\n",
              "      <td>-6.090047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.664802</td>\n",
              "      <td>-0.237792</td>\n",
              "      <td>1.422423</td>\n",
              "      <td>0.466005</td>\n",
              "      <td>2.051005</td>\n",
              "      <td>0.628103</td>\n",
              "      <td>-4.922358</td>\n",
              "      <td>0.204586</td>\n",
              "      <td>-2.880477</td>\n",
              "      <td>1.596624</td>\n",
              "      <td>0.843941</td>\n",
              "      <td>0.500198</td>\n",
              "      <td>-1.591305</td>\n",
              "      <td>0.053384</td>\n",
              "      <td>1.662664</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.920713</td>\n",
              "      <td>0.744035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.443205</td>\n",
              "      <td>1.203531</td>\n",
              "      <td>-0.523704</td>\n",
              "      <td>1.272438</td>\n",
              "      <td>1.110078</td>\n",
              "      <td>-0.785318</td>\n",
              "      <td>-1.270623</td>\n",
              "      <td>3.521826</td>\n",
              "      <td>-3.037687</td>\n",
              "      <td>0.292205</td>\n",
              "      <td>3.863921</td>\n",
              "      <td>0.058702</td>\n",
              "      <td>1.856966</td>\n",
              "      <td>-1.365796</td>\n",
              "      <td>-0.063612</td>\n",
              "      <td>1</td>\n",
              "      <td>6.651231</td>\n",
              "      <td>0.371812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.031723</td>\n",
              "      <td>-2.980087</td>\n",
              "      <td>-0.571903</td>\n",
              "      <td>0.527606</td>\n",
              "      <td>-4.355382</td>\n",
              "      <td>1.658870</td>\n",
              "      <td>0.028287</td>\n",
              "      <td>1.207216</td>\n",
              "      <td>-0.963811</td>\n",
              "      <td>0.245698</td>\n",
              "      <td>-0.699508</td>\n",
              "      <td>-1.290597</td>\n",
              "      <td>-2.561109</td>\n",
              "      <td>-0.641112</td>\n",
              "      <td>-0.192754</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.826286</td>\n",
              "      <td>0.129632</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
              "0   0.391943   1.386619  -0.140178   0.388745  -3.054169  -0.093261   \n",
              "1   0.921189  -1.972131   0.073887  -1.372257  -2.172764   0.301242   \n",
              "2   3.664802  -0.237792   1.422423   0.466005   2.051005   0.628103   \n",
              "3  -1.443205   1.203531  -0.523704   1.272438   1.110078  -0.785318   \n",
              "4  -0.031723  -2.980087  -0.571903   0.527606  -4.355382   1.658870   \n",
              "\n",
              "   feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
              "0  -1.709212   2.857931   0.356881  -2.542477    0.155455   -1.306018   \n",
              "1   1.104396  -0.048902   0.896611   4.437979   -2.222714   -2.141050   \n",
              "2  -4.922358   0.204586  -2.880477   1.596624    0.843941    0.500198   \n",
              "3  -1.270623   3.521826  -3.037687   0.292205    3.863921    0.058702   \n",
              "4   0.028287   1.207216  -0.963811   0.245698   -0.699508   -1.290597   \n",
              "\n",
              "   feature_12  feature_13  feature_14  target  positive_predictors_sum  \\\n",
              "0    0.063715    1.172275   -1.302984       1                 3.310391   \n",
              "1   -2.381243    2.439874    1.146494       1                -3.802402   \n",
              "2   -1.591305    0.053384    1.662664       1                -0.920713   \n",
              "3    1.856966   -1.365796   -0.063612       1                 6.651231   \n",
              "4   -2.561109   -0.641112   -0.192754       0                -0.826286   \n",
              "\n",
              "   main_interaction  \n",
              "0         -0.988375  \n",
              "1         -6.090047  \n",
              "2          0.744035  \n",
              "3          0.371812  \n",
              "4          0.129632  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_featured = pd.read_csv('dataset_cleaned.csv')\n",
        "\n",
        "# 1. Create the sum of the strongest positive predictors\n",
        "df_featured['positive_predictors_sum'] = df_featured[['feature_3', 'feature_7', 'feature_12']].sum(axis=1)\n",
        "\n",
        "# 2. Create the interaction term from the strongest positive and negative predictors\n",
        "df_featured['main_interaction'] = df_featured['feature_3'] * df_featured['feature_9']\n",
        "\n",
        "print(\"Successfully created two new, data-driven features.\")\n",
        "df_featured.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSWzkazNsO4A",
        "outputId": "3787b187-dac6-45f4-aa96-c1fd5697f1d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final featured dataset saved to 'dataset_featured.csv'\n"
          ]
        }
      ],
      "source": [
        "df_featured.to_csv('dataset_featured.csv', index=False)\n",
        "\n",
        "print(\"\\nFinal featured dataset saved to 'dataset_featured.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9q2mKzxsVNB"
      },
      "source": [
        "## 4. Modeling\n",
        "\n",
        "With our data cleaned and our features engineered, it's time to train machine learning algorithms on our prepared data so they can learn the patterns that distinguish between the two target classes.\n",
        "\n",
        "We will train two different and popular classification models:\n",
        "\n",
        "1. Logistic Regression: A fast, simple, and highly interpretable linear model. It's a great baseline to see how well a straightforward approach works.\n",
        "\n",
        "2. Random Forest: A powerful and complex ensemble model that uses multiple \"decision trees\" to make a final prediction. It can capture non-linear relationships and is often a top performer.\n",
        "\n",
        "By comparing these two, we can see if the complexity of the Random Forest provides a real benefit over the simplicity of Logistic Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "Pd0nKM4zsOuy",
        "outputId": "5c4bfca5-8ce8-4f0f-d834-77eaecd900a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Featured dataset loaded successfully.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_0</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>feature_9</th>\n",
              "      <th>feature_10</th>\n",
              "      <th>feature_11</th>\n",
              "      <th>feature_12</th>\n",
              "      <th>feature_13</th>\n",
              "      <th>feature_14</th>\n",
              "      <th>target</th>\n",
              "      <th>positive_predictors_sum</th>\n",
              "      <th>main_interaction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.391943</td>\n",
              "      <td>1.386619</td>\n",
              "      <td>-0.140178</td>\n",
              "      <td>0.388745</td>\n",
              "      <td>-3.054169</td>\n",
              "      <td>-0.093261</td>\n",
              "      <td>-1.709212</td>\n",
              "      <td>2.857931</td>\n",
              "      <td>0.356881</td>\n",
              "      <td>-2.542477</td>\n",
              "      <td>0.155455</td>\n",
              "      <td>-1.306018</td>\n",
              "      <td>0.063715</td>\n",
              "      <td>1.172275</td>\n",
              "      <td>-1.302984</td>\n",
              "      <td>1</td>\n",
              "      <td>3.310391</td>\n",
              "      <td>-0.988375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.921189</td>\n",
              "      <td>-1.972131</td>\n",
              "      <td>0.073887</td>\n",
              "      <td>-1.372257</td>\n",
              "      <td>-2.172764</td>\n",
              "      <td>0.301242</td>\n",
              "      <td>1.104396</td>\n",
              "      <td>-0.048902</td>\n",
              "      <td>0.896611</td>\n",
              "      <td>4.437979</td>\n",
              "      <td>-2.222714</td>\n",
              "      <td>-2.141050</td>\n",
              "      <td>-2.381243</td>\n",
              "      <td>2.439874</td>\n",
              "      <td>1.146494</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.802402</td>\n",
              "      <td>-6.090047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.664802</td>\n",
              "      <td>-0.237792</td>\n",
              "      <td>1.422423</td>\n",
              "      <td>0.466005</td>\n",
              "      <td>2.051005</td>\n",
              "      <td>0.628103</td>\n",
              "      <td>-4.922358</td>\n",
              "      <td>0.204586</td>\n",
              "      <td>-2.880477</td>\n",
              "      <td>1.596624</td>\n",
              "      <td>0.843941</td>\n",
              "      <td>0.500198</td>\n",
              "      <td>-1.591305</td>\n",
              "      <td>0.053384</td>\n",
              "      <td>1.662664</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.920713</td>\n",
              "      <td>0.744035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.443205</td>\n",
              "      <td>1.203531</td>\n",
              "      <td>-0.523704</td>\n",
              "      <td>1.272438</td>\n",
              "      <td>1.110078</td>\n",
              "      <td>-0.785318</td>\n",
              "      <td>-1.270623</td>\n",
              "      <td>3.521826</td>\n",
              "      <td>-3.037687</td>\n",
              "      <td>0.292205</td>\n",
              "      <td>3.863921</td>\n",
              "      <td>0.058702</td>\n",
              "      <td>1.856966</td>\n",
              "      <td>-1.365796</td>\n",
              "      <td>-0.063612</td>\n",
              "      <td>1</td>\n",
              "      <td>6.651231</td>\n",
              "      <td>0.371812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.031723</td>\n",
              "      <td>-2.980087</td>\n",
              "      <td>-0.571903</td>\n",
              "      <td>0.527606</td>\n",
              "      <td>-4.355382</td>\n",
              "      <td>1.658870</td>\n",
              "      <td>0.028287</td>\n",
              "      <td>1.207216</td>\n",
              "      <td>-0.963811</td>\n",
              "      <td>0.245698</td>\n",
              "      <td>-0.699508</td>\n",
              "      <td>-1.290597</td>\n",
              "      <td>-2.561109</td>\n",
              "      <td>-0.641112</td>\n",
              "      <td>-0.192754</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.826286</td>\n",
              "      <td>0.129632</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
              "0   0.391943   1.386619  -0.140178   0.388745  -3.054169  -0.093261   \n",
              "1   0.921189  -1.972131   0.073887  -1.372257  -2.172764   0.301242   \n",
              "2   3.664802  -0.237792   1.422423   0.466005   2.051005   0.628103   \n",
              "3  -1.443205   1.203531  -0.523704   1.272438   1.110078  -0.785318   \n",
              "4  -0.031723  -2.980087  -0.571903   0.527606  -4.355382   1.658870   \n",
              "\n",
              "   feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
              "0  -1.709212   2.857931   0.356881  -2.542477    0.155455   -1.306018   \n",
              "1   1.104396  -0.048902   0.896611   4.437979   -2.222714   -2.141050   \n",
              "2  -4.922358   0.204586  -2.880477   1.596624    0.843941    0.500198   \n",
              "3  -1.270623   3.521826  -3.037687   0.292205    3.863921    0.058702   \n",
              "4   0.028287   1.207216  -0.963811   0.245698   -0.699508   -1.290597   \n",
              "\n",
              "   feature_12  feature_13  feature_14  target  positive_predictors_sum  \\\n",
              "0    0.063715    1.172275   -1.302984       1                 3.310391   \n",
              "1   -2.381243    2.439874    1.146494       1                -3.802402   \n",
              "2   -1.591305    0.053384    1.662664       1                -0.920713   \n",
              "3    1.856966   -1.365796   -0.063612       1                 6.651231   \n",
              "4   -2.561109   -0.641112   -0.192754       0                -0.826286   \n",
              "\n",
              "   main_interaction  \n",
              "0         -0.988375  \n",
              "1         -6.090047  \n",
              "2          0.744035  \n",
              "3          0.371812  \n",
              "4          0.129632  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_model = pd.read_csv('dataset_featured.csv')\n",
        "\n",
        "print(\"Featured dataset loaded successfully.\")\n",
        "df_model.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwLhR5nlseCH"
      },
      "source": [
        "Preparing Data for Training (**Train-Test Split**)\n",
        "\n",
        "Before we can train a model, we must decide on the train-test split. we can do a 80-20 split.\n",
        "\n",
        "First, we'll separate our data into X (the features) and y (the target variable we want to predict)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO1I5uIasOmc",
        "outputId": "f0a05129-a7b6-4137-9f85-df6ed0d39df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features (X) shape: (1000, 17)\n",
            "Target (y) shape: (1000,)\n",
            "Training set size: 800\n",
            "Testing set size: 200\n"
          ]
        }
      ],
      "source": [
        "X = df_model.drop('target', axis=1)\n",
        "y = df_model['target']\n",
        "\n",
        "print(\"Features (X) shape:\", X.shape)\n",
        "print(\"Target (y) shape:\", y.shape)\n",
        "\n",
        "# Use the train_test_split function to divide the data\n",
        "# test_size=0.2 means 20% of the data will be used for testing\n",
        "# random_state=42 ensures that the split is the same every time we run the code, making our results reproducible\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training set size:\", len(X_train))\n",
        "print(\"Testing set size:\", len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4lkUKMlsm9y"
      },
      "source": [
        "Initialize and Train the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UVgFy1lsOXY",
        "outputId": "93fb1252-b6b6-457e-f142-c9c5b5a35375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression model trained successfully.\n",
            "Random Forest model trained successfully.\n"
          ]
        }
      ],
      "source": [
        "# 1. Initialize the Logistic Regression model\n",
        "# We set max_iter=1000 to ensure the model has enough iterations to converge\n",
        "log_reg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "# 2. Initialize the Random Forest Classifier model\n",
        "rand_forest_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 3. Train the Logistic Regression model on the training data\n",
        "log_reg_model.fit(X_train, y_train)\n",
        "print(\"Logistic Regression model trained successfully.\")\n",
        "\n",
        "# 4. Train the Random Forest model on the training data\n",
        "rand_forest_model.fit(X_train, y_train)\n",
        "print(\"Random Forest model trained successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsttcADnsyV2"
      },
      "source": [
        "Make Predictions on the Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJBmod61ssFg",
        "outputId": "031441a5-a986-43cb-ca05-67e8251c9d2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions generated successfully for both models.\n"
          ]
        }
      ],
      "source": [
        "# Make predictions with the Logistic Regression model\n",
        "y_pred_log_reg = log_reg_model.predict(X_test)\n",
        "\n",
        "# Make predictions with the Random Forest model\n",
        "y_pred_rand_forest = rand_forest_model.predict(X_test)\n",
        "\n",
        "# To calculate ROC-AUC, we also need the probability predictions\n",
        "y_prob_log_reg = log_reg_model.predict_proba(X_test)[:, 1]\n",
        "y_prob_rand_forest = rand_forest_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Predictions generated successfully for both models.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBA4cHLcs4xc"
      },
      "source": [
        "## 5. Evalution\n",
        "\n",
        "We'll use four standard metrics for classification:\n",
        "\n",
        "1. Accuracy: The most intuitive metric. It's the percentage of total predictions that were correct. $\\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Predictions}}$\n",
        "\n",
        "\n",
        "2. Precision: Of all the times the model predicted \"Positive\" (class 1), what percentage were actually positive? This metric is important when the cost of a false positive is high. $\\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$\n",
        "\n",
        "\n",
        "3. F1-Score: The harmonic mean of Precision and Recall. It provides a single score that balances both concerns, making it a great overall measure, especially if the classes are imbalanced (though ours are not).\n",
        "\n",
        "4. ROC-AUC Score: This measures the model's ability to distinguish between the positive and negative classes across all possible thresholds. An AUC of 1.0 is a perfect classifier, while 0.5 is no better than random guessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4hh4Yjpsr59",
        "outputId": "cf516e56-8462-40dd-b6d4-f10654ac536e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Logistic Regression Evaluation ---\n",
            "  Accuracy: 0.8950\n",
            "  Precision: 0.8763\n",
            "  F1-score: 0.8901\n",
            "  ROC-AUC: 0.9578\n",
            "\n",
            "--- Random Forest Evaluation ---\n",
            "  Accuracy: 0.9450\n",
            "  Precision: 0.9560\n",
            "  F1-score: 0.9405\n",
            "  ROC-AUC: 0.9836\n",
            "\n",
            "--- Model Comparison ---\n",
            "           Logistic Regression  Random Forest\n",
            "Metric                                       \n",
            "Accuracy              0.895000       0.945000\n",
            "Precision             0.876289       0.956044\n",
            "F1-score              0.890052       0.940541\n",
            "ROC-AUC               0.957848       0.983641\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Logistic Regression Evaluation ---\")\n",
        "acc_lr = accuracy_score(y_test, y_pred_log_reg)\n",
        "prec_lr = precision_score(y_test, y_pred_log_reg)\n",
        "f1_lr = f1_score(y_test, y_pred_log_reg)\n",
        "roc_auc_lr = roc_auc_score(y_test, y_prob_log_reg)\n",
        "print(f\"  Accuracy: {acc_lr:.4f}\")\n",
        "print(f\"  Precision: {prec_lr:.4f}\")\n",
        "print(f\"  F1-score: {f1_lr:.4f}\")\n",
        "print(f\"  ROC-AUC: {roc_auc_lr:.4f}\")\n",
        "\n",
        "print(\"\\n--- Random Forest Evaluation ---\")\n",
        "acc_rf = accuracy_score(y_test, y_pred_rand_forest)\n",
        "prec_rf = precision_score(y_test, y_pred_rand_forest)\n",
        "f1_rf = f1_score(y_test, y_pred_rand_forest)\n",
        "roc_auc_rf = roc_auc_score(y_test, y_prob_rand_forest)\n",
        "print(f\"  Accuracy: {acc_rf:.4f}\")\n",
        "print(f\"  Precision: {prec_rf:.4f}\")\n",
        "print(f\"  F1-score: {f1_rf:.4f}\")\n",
        "print(f\"  ROC-AUC: {roc_auc_rf:.4f}\")\n",
        "\n",
        "# --- Create a comparison table ---\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'F1-score', 'ROC-AUC'],\n",
        "    'Logistic Regression': [acc_lr, prec_lr, f1_lr, roc_auc_lr],\n",
        "    'Random Forest': [acc_rf, prec_rf, f1_rf, roc_auc_rf]\n",
        "})\n",
        "print(\"\\n--- Model Comparison ---\")\n",
        "print(metrics_df.set_index('Metric'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsoCax11tEQJ"
      },
      "source": [
        "## 6. Overfitting Analysis and Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8B2Pzd0srrP",
        "outputId": "e4bbf8d3-4fb1-40b1-ee4d-bb252e4cedcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Overfitting Analysis ---\n",
            "Logistic Regression Training Accuracy: 0.8825\n",
            "Logistic Regression Test Accuracy:   0.8950\n",
            "\n",
            "Random Forest Training Accuracy:     1.0000\n",
            "Random Forest Test Accuracy:       0.9450\n",
            "\n",
            "--- Cross-Validation Scores (Accuracy) ---\n",
            "Logistic Regression CV Mean Accuracy: 0.8770 +/- 0.0087\n",
            "Random Forest CV Mean Accuracy:       0.9300 +/- 0.0152\n"
          ]
        }
      ],
      "source": [
        "# --- Overfitting Check ---\n",
        "train_acc_lr = log_reg_model.score(X_train, y_train)\n",
        "train_acc_rf = rand_forest_model.score(X_train, y_train)\n",
        "\n",
        "print(\"--- Overfitting Analysis ---\")\n",
        "print(f\"Logistic Regression Training Accuracy: {train_acc_lr:.4f}\")\n",
        "print(f\"Logistic Regression Test Accuracy:   {acc_lr:.4f}\\n\")\n",
        "\n",
        "print(f\"Random Forest Training Accuracy:     {train_acc_rf:.4f}\")\n",
        "print(f\"Random Forest Test Accuracy:       {acc_rf:.4f}\")\n",
        "\n",
        "\n",
        "# --- 5-Fold Cross-Validation ---\n",
        "print(\"\\n--- Cross-Validation Scores (Accuracy) ---\")\n",
        "\n",
        "# Perform 5-fold cross-validation on the entire dataset (X, y)\n",
        "cv_scores_lr = cross_val_score(log_reg_model, X, y, cv=5)\n",
        "cv_scores_rf = cross_val_score(rand_forest_model, X, y, cv=5)\n",
        "\n",
        "print(f\"Logistic Regression CV Mean Accuracy: {cv_scores_lr.mean():.4f} +/- {cv_scores_lr.std():.4f}\")\n",
        "print(f\"Random Forest CV Mean Accuracy:       {cv_scores_rf.mean():.4f} +/- {cv_scores_rf.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IlJBFkntNUp"
      },
      "source": [
        "## conclusions from above metrics:\n",
        "\n",
        "1. **Model Performance:** The Random Forest model is the decisive winner. Based on the test set evaluation, it achieved a superior Accuracy of 94.5% and an excellent ROC-AUC score of 0.9836. This significantly surpasses the Logistic Regression model's 89.5% accuracy and 0.9578 ROC-AUC, proving that the Random Forest's ability to capture complex patterns was more effective for this dataset.\n",
        "\n",
        "2. **Overfitting:** The results clearly illustrate the classic trade-off between model power and overfitting. The Random Forest model achieved a perfect 1.0000 training accuracy, which, when compared to its 0.9450 test accuracy, is a definitive sign of overfitting; it has memorized the training data to some extent. In contrast, the Logistic Regression model showed no signs of overfitting, with its test accuracy (0.8950) being very close to its training accuracy (0.8825), indicating it generalizes well.\n",
        "\n",
        "3. **Robustness (Cross-Validation):** The 5-fold cross-validation scores provide the most reliable estimate of real-world performance. They confirm the Random Forest's superiority, giving it a robust average accuracy of 93.0%. This is slightly lower than the single test split result but still excellent. The cross-validation also reinforces that the Logistic Regression model is stable and consistent, with its average accuracy of 87.7% closely matching its performance in the other tests. Ultimately, while the Random Forest overfits to a degree, its generalized performance is still substantially better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBhv-2G5tKtI",
        "outputId": "2167885c-cf3b-44f0-950f-f4a5f3b7a7a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final model saved successfully as 'final_model.joblib'\n"
          ]
        }
      ],
      "source": [
        "final_model = rand_forest_model\n",
        "\n",
        "joblib.dump(final_model, 'final_model.joblib')\n",
        "\n",
        "print(\"Final model saved successfully as 'final_model.joblib'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJScy5aWtVCt"
      },
      "source": [
        "After a thorough process of data cleaning, feature engineering, and competitive evaluation, the Random Forest classifier was definitively selected as the champion model. It significantly outperformed the Logistic Regression, achieving a strong test set accuracy of 94.5% and a robust cross-validated accuracy of 93.0%. While the model showed signs of overfitting by perfectly memorizing the training data, its exceptional performance on unseen test data confirmed its superior predictive power. This final, trained Random Forest model has now been saved to the file final_model.joblib, concluding the development phase and making it ready for our next major goal: deployment."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
